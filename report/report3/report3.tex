\documentclass{ujarticle} %upLaTeXで動いているのでunicode対応のこのクラス
\usepackage{xcolor}
\usepackage[dvipdfmx]{graphicx}
\usepackage{float}
\usepackage{ascmac}
% todo command
\newcommand\todo[1]{\textcolor{red}{[todo] #1}}
\newcommand\tips[1]{\textcolor{blue}{#1}}

\begin{document}
\title{計算機科学実験及演習4 \\ \bf 画像認識 第３回レポート}
% ↓ここに自分の氏名を記入
\author{谷　勇輝 \\ \\入学年 平成27年 \\ 学籍番号 1029-27-2870}
\西暦
\date{締切日: 2018年1月19日\\ 提出日: 2018年1月12日}
\maketitle
\newpage

% レポートには，課題内容，作成したプログラムの説明，実行結果，工夫点，問題点を記述して下さい．これとは別にソースファイルを添付して下さい．レポートは報告書の形式を要求します．くだけた文体，支離滅裂な文章については低評価となります

\begin{itembox}[l]{[課題 3] 誤差逆伝播法による 3 層ニューラルネットワークの学習}

[課題2]のコードをベースに，3層ニューラルネットワークのパラメータW(1), W(2), b(1), b(2)を学習するプログラムを作成せよ．

\begin{itemize}
  \item ネットワークの構造，バッチサイズは課題 2 と同じで良い．
  \item 学習には MNIST の学習データ 60000 枚を用いること．
  \item 繰り返し回数は自由に決めて良い．教師データの数をN,ミニバッチのサイズをBとしたとき，N/B 回の繰り返しを 1 エポックと呼び，通常はエポック数とミニバッチサイズで繰り返し回数を指定する．
  \item 学習率 η は自由に決めて良い（0.01 あたりが経験的に良さそう）
  \item 各エポックの処理が終了する毎に，クロスエントロピー誤差を標準出力に出力すること．（これにより学習がうまく進んでいるかどうかを確認することができる）
  \item 学習終了時に学習したパラメータをファイルに保存する機能を用意すること．フォーマットは自由．パラメータを numpy の配列 (ndarray) で実装している場合はnumpy.save() や numpy.savez() 関数が利用できる．
  \item ファイルに保存したパラメータを読み込み，再利用できる機能を用意すること．（numpy.load()関数が利用できる）
\end{itemize}

\end{itembox}

\section{作成したプログラム}
\subsection{プログラム構成}
今回作成したプログラムは以下のモジュール、クラスからなる。

オブジェクト指向・ドメインモデルに基づいた適切なモジュール設計により、今後の開発がよりスムーズになるよう注意した。

\begin{verbatim}
neural_network
 ┣ ioex.py            … 入出力系モジュール
 ┃ ┣ InputManager       … 入力を担当するクラス
 ┃ ┗ OutputManager      … 出力を担当するクラス
 ┣ data.py            … プログラムで扱う基本データ系モジュール
 ┃ ┣ MnistDataBox       … MNISTデータ群を表すクラス
 ┃ ┗ MnistData          … 単一のMNISTデータを表すクラス
 ┣ layer.py           … ニューラルネットの層モジュール
 ┃ ┣ Layer              … 基本の層構造を表すベースクラス
 ┃ ┣ InputLayer         … 入力層を表すクラス
 ┃ ┣ ConversionLayer    … 変換を行う層を表すクラス
 ┃ ┣ HiddenLayer        … 中間層を表すクラス
 ┃ ┗ OutputLayer        … 出力層を表すクラス
 ┣ util.py            … 汎用関数や汎用クラスを集めたモジュール
 ┃ ┣ ComplexMaker       … numpyのrandomを扱うクラス
 ┃ ┗ 各種数学メソッド群
 ┣ nn.py              … 組み合わせ済ニューラルネットのモジュール
 ┃ ┗ NeuralNetwork3Layers　… 3層ニューラルネットのクラス
 ┣ learning.py        … 学習に関係する関数を集めたモジュール
 ┃ ┗ 逆伝播に関係するメソッド群
 ┣ test.py            … 動作確認・テスト用モジュール
 ┣ task1.py           … 課題１プログラム
 ┣ task2.py           … 課題２プログラム
 ┗ task3.py           … 課題３プログラム
MNIST
\end{verbatim}

課題２から追加したのは、learning.pyモジュールとtask3.pyプログラムである。learning.pyモジュールには、学習の際に使用する逆伝播関数を集約している。task3.pyは課題３のメインプログラムである。

また、ニューラルネットを学習できるようにするため、既存のモジュールの各クラスにも必要なメソッドを追加した。

\subsection{メインプログラム}

課題３のメインプログラムtest3.pyは以下の通りである。

プログラムを起動すると、CUIによって５つの入力を順に求められる。

１つ目の入力はニューラルネットの値の初期値として外部ファイルを使用するかどうかである。yesと返答した場合は neuralNetwork/learned_data/フォルダ内の４つの.npyファイル、「hiddenWeight.npy」「hiddenShift.npy」「outputWeight.npy」「outputShift.npy」からパラメータが抽出され、ニューラルネットにセットされる。noと返答した場合はランダムにニューラルネット内のパラメータが設定される。

２つ目、３つ目の入力はそれぞれ学習用入力データのバッチサイズとエポック数である。バッチサイズを大きくするほど１セットの学習における値の更新回数は少なくなり、学習時間も短くなるが、学習精度が悪くなる。エポック数は学習セットの回数である。

４つ目の入力は学習用入力バッチの内容を決定する乱数シードである。この値を元に学習バッチデータがランダムに決まるため、同一のシードを指定すれば同一の学習バッチセットが入力される。

５つ目の入力は学習率である。単位の接頭辞はミリであり、この値を大きくするほど１つのバッチデータでの１回の学習の影響が大きくなる。

学習中は学習用バッチ更新されるたびに、現在のパラメータでそのバッチを入力した際のクロスエントロピー誤差、正答率を出力する。課題用件ではエポック終了の度にクロスエントロピー誤差を表示するように指定されていたが、プログラムが正確に動いているのを確認するためにこのような仕様とした。また、当エポックでのこれまでののべ使用MNISTデータ数もリアルタイムに表示する。これにより現在の実行進行率を確認することができる。

また、全エポックが終了した後に、今までの正答率の平均を表示する。

最後に、１つの入力を求められる。その入力により現在のパラメータを外部ファイルに保存するかどうかが決まる。

\begin{itembox}[l]{test3.py}
  \begin{verbatim}
1   import ioex (以下import省略)
5
6   print("### task3 ###")
7
8   # 入出力準備
9   inputM = ioex.InputManager()
10  outputM = ioex.OutputManager()
11  trainingData = inputM.getMnistTrainingData()
12
13  neuralNet = nn.NeuralNetwork3Layers(28*28, 50, 10)
14
15  print("---- load section ----")
16  print("use weight and shift data file?")
17  useFile = inputM.selectYesOrNo()
18  if useFile :
19      neuralNet.load()
20
21  print("---- learning section ----")
22  print("input batch size")
23  batchSize = inputM.selectNumber()
24
25  epoch = int(trainingData.images.shape[0] / batchSize)
26
27  print("input repeat epoch")
28  repeatEpoch = inputM.selectNumber()
29
30  print("input seed")
31  seed = inputM.selectNumber()
32
33  print("input update ratio (mili)")
34  updateRatio = inputM.selectNumber() * 0.001
\end{verbatim}
\end{itembox}

\begin{itembox}[l]{test3.py 続き}
  \begin{verbatim}
35
36  for e in range(0,repeatEpoch) :
37      print("---- epoch "+str(e+1)+" ----")
38      total = 0
39      for i in range(0,epoch) :
40          neuralNet.learn(trainingData, batchSize,
                            seed, updateRatio)
41          print(neuralNet.getLoss(),
                  end = " ")
42          percentOfCurrent =
                  neuralNet.getPersentageOfCurrent()
43          print(str(percentOfCurrent) + "% (" +
                  str((i*batchSize))  +")")
44          total = total + percentOfCurrent / epoch
45      print("total percent of current answer : " +
              str(total) + "%")
46
47  print("finish.")
48
49  print("---- save section ----")
50  print("save?")
51  save = inputM.selectYesOrNo()
52
53  if save :
54      neuralNet.save()
55
56  print("Bye.")
  \end{verbatim}
\end{itembox}

16行目では、MNISTデータ群の順番をランダムに変更するメソッドMnistDataBox\#shuffleを呼び出している。これは後の課題で複数回バッチデータによる学習を行う際に、入力バッチデータを変更するためのメソッドである。18行目では３層ニューラルネットワークの作成を行っている。NeuralNetwork3Layersクラスのコンストラクタの第１引数はInputLayerの次元数、第２引数はHiddenLayerの次元数、第３引数はOutputLayerの次元数である。各Layerは生成された時にコンストラクタによって初期化され、その際に各重みの値がランダムに決定される。

入力後のプログラムの流れを示す。まず24,25行目のMnistDataBox\# getImageBatchメソッド,MnistDataBox\#getAnswerVecotrBatchメソッド によって入力バッチとそれに対応するラベルバッチを取得する。いずれのメソッドも第１引数は取得するバッチサイズ、第２引数はバッチ取得の際に使用する基準位置を指定する。次に、26,27行目のNeuralNetwork3Layers\#setInputBatchメソッド,NeuralNetwork3Layers\#setAnswerBatchメソッドによって入力バッチとラベルバッチをニューラルネットにセットする。続いて、30行目のNeuralNetwork3Layers\#calculateメソッドで入力バッチに基づいてニューラルネット内の計算を行う。最後に32行目のNeuralNetwork3Layers\#getLossメソッドによって 出力とラベルバッチからクロスエントロピー誤差を計算している。

%############################################
\subsection{nnモジュール}
構成済みのニューラルネットワークを提供するモジュール。メインプログラムではこのnnモジュール内のクラスより下位のモジュールやクラス（layerモジュール等）を認知、操作する必要はないようモジュール作成をした。

\subsubsection{NeuralNetwork3Layersクラス}
構成済み３層ニューラルネットワークのクラス。Layerモジュールのメソッドを隠蔽する。また、ラベルデータの保持、管理を行っている。

\begin{itembox}[l]{nn.py}
  \begin{verbatim}
 class NeuralNetwork3Layers :
    def __init__(self, inputDim, hiddenDim, outputDim):
        # 三層ニューラルネットワーク
        self.inputLayer = layer.InputLayer(inputDim)
        self.hiddenLayer =
          layer.HiddenLayer(hiddenDim,self.inputLayer)
        self.outputLayer =
          layer.OutputLayer(outputDim,self.hiddenLayer)
        # 正解データ
        self.answer = np.zeros(outputDim)

    ## 入力系メソッド ###

    def setInput(self, inputData):
        self.inputLayer.setInput(inputData)

    def setInputBatch(self, inputBatch):
        self.inputLayer.setInputBatch(inputBatch)

    def setAnswer(self, answerVector):
        self.answer =
          answerVector.reshape(answerVector.size,1)
        if(self.answer.size != self.outputLayer.dimension):
            print("WARNING!
              Input data is NOT match to deimension")

    def setAnswerBatch(self,answerBatch):
        self.answer = answerBatch

    ## 活性メソッド ##

    def calculate(self):
        return self.outputLayer.calculate()

    def getLoss(self):
        return self.outputLayer.getLoss(self.answer)
  \end{verbatim}
\end{itembox}

主なメソッドは以下の通りである。
\begin{itemize}
  \item \_\_init\_\_(self, inputDim, hiddenDim, outputDim)

  インスタンスを作成し値を初期化する。３つの層に含まれるニューロンの数(次元数)をここで決定する。

  \item setInput(self, inputData)

  単一入力をニューラルネットにセットする。

  \item setInputBatch(self, inputBatch)

  バッチ入力をニューラルネットにセットする。

  \item setAnswer(self, answerVector)

  単一ラベルデータをニューラルネットにセットする。

  \item setAnswerBatch(self,answerBatch)

  バッチラベルデータをニューラルネットにセットする。

  \item calculate(self)

  現在セットされている入力に基づいてニューラルネットを活性化させ順方向伝播を行う。

  \item getLoss(self)

  現在の出力とセットされているラベルデータに基づいてクロスエントロピー誤差平均を算出する。

\end{itemize}

%#################################################
\subsection{layerモジュール}
ニューラルネットワークの部品となるlayerモジュール内のクラスはこのプログラム群において重要な役割を果たしている。その詳細を以下に示す。

%#####
\subsubsection{Layerクラス}
このモジュール内の全てのLayerクラスのベースクラスであり、他のLayerはこのクラスを必ず継承する。層についての基本の機能を備えている。

課題２から追加されたインスタンス変数、メソッドは以下の通りである。

\begin{itemize}
  \item self.lossFunction

  誤差計算に使用する関数を格納するインスタンス変数。初期設定ではクロスエントロピー誤差関数がセットされている。

  \item getLoss(self, answer)

  現在の出力値と引数に指定したラベルデータを元に、self.lossFunctionに格納された関数を使用して計算を行い、誤差の値を返す。

\end{itemize}

その他のインスタンス変数、メソッドは以下の通りである。

\begin{itemize}
  \item self.dimension

  層に含まれるニューロンの数、すなわち層の次元数を表す。コンストラクタの引数によって初期化される。

  \item self.output

  層の出力を表す。caluculateメソッドによって更新され、現在の最新の出力情報を保持する。初期値は全て０である。

  \item \_\_init\_\_(self, dimension)

  インスタンスを作成し値を初期化する。層に含まれるニューロンの数(次元数)はここで決定する。

  \item calculate(self)

  現在の最新の情報を使用して出力を更新する。更新後の値を返す。

  \item getOutput(self)

  現在の最新の出力情報を返す。このメソッドでは出力値の更新は行われない。

  \item confirmParameters(self)

  層についての情報を標準出力に表示する。

  \item getDimension(self)

  層が含むニューロンの数、すなわち次元数を返す。
\end{itemize}

%#####
\subsubsection{InputLayerクラス}
入力層を表すクラス。Layerクラスを継承する。

課題２から追加されたメソッドは以下の通りである。

\begin{itemize}
  \item setInputBatch(self, inputBatch)

  バッチ入力をニューラルネットにセットする。
\end{itemize}

その他のインスタンス変数、メソッドは以下の通りである。
\begin{itemize}
  \item self.input

  入力を表す変数。初期値は全て０である。setInputメソッドによって設定する。

  \item setInput(self, inputData)

  入力を設定する。引数に与えられた配列は１次元に圧縮される。

  \item calculate(self)

  Layerクラスのメソッドをオーバーライドしている。現在の入力の値をそのまま出力として設定する。
\end{itemize}

%#####
\subsubsection{ConversionLayerクラス}
入力に対し重みをつけ、さらに何らかの活性関数を適応したものを出力とする層、すなわち何らかの変換を行う層を表す。具体的な実装としては後に示すHiddenLayerやOutputLayerがある。

主なインスタンス変数、メソッドは以下の通りである。
\begin{itemize}
  \item self.prevLayer

  直前の層のインスタンスを保持する。コンストラクタの引数によって初期化される。

  \item self.weightSize

  入力に掛けられる重み行列のサイズを表したタプル。前の層の次元情報とこの層の次元情報から自動的に初期化される。

  \item self.shiftSize

  入力に加算される閾値ベクトルのサイズを表したタプル。この層の次元情報から自動的に初期化される。

  \item self.weight

  入力に掛けられる重み行列を表す変数。このクラスでは単位行列に初期化される。

  \item self.shift

  入力に加算される閾値ベクトルを表す変数。このクラスではゼロベクトルに初期化される。

  \item self.activator

  重み付け後に適応される活性関数をあらわす変数。このクラスでは恒等変換に初期化される。

  \item calculate(self)

  Layerクラスのメソッドをオーバーライドしている。まず前の層のcalculateを行い、その最新の出力を入力として受け取る。その入力にself.weightをかけ、self.shiftを加算することで重み付けを行う。最後に活性関数self.activatorを適応し得られた値を最新の出力として更新する。また、更新後の値を返す。この再帰的なcalculateの呼び出しにより、ニューラルネットワークの最後の層のcalculateを呼び出すことでネットワーク全ての値を順方向に更新できることになる。

  \item setWeight(self, weight)

  重み行列self.weightを設定する。この層に合わない形の行列が入力された場合には設定は行われず、警告文が標準出力に表示される。

  \item setShift(self, shift)

  閾値ベクトルself.shiftを設定する。この層に合わない形のベクトルが入力された場合には設定は行われず、警告文が標準出力に表示される。

  \item setActivator(self, function)

  活性化関数self.activatorを設定する。関数型以外の値が入力された場合には設定は行われず、警告文が標準出力に表示される。
\end{itemize}

%#####
\subsubsection{HiddenLayerクラス}
中間層を表すクラス。変換を行う層であるので、ConversionLayerを継承する。

ConversionLayerとの違いは、その初期化の内容である。こちらのクラスでは、コンストラクタで２つの乱数シードweightSeedとshiftSeedを要求し（デフォルトの値は１）、その乱数シードを使って生成したランダム値に基づいて重み行列self.weightと閾値ベクトルself.shiftの初期値が設定される。また、活性化関数self.activatorは\textbf{シグモイド関数}に初期化される。

\subsubsection{OutputLayerクラス}
出力層を表すクラス。変換を行う層であるので、ConversionLayerを継承する。

ConversionLayerとの違いは、その初期化の内容である。こちらのクラスでは、コンストラクタで２つの乱数シードweightSeedとshiftSeedを要求し（デフォルトの値は１）、その乱数シードを使って生成したランダム値に基づいて重み行列self.weightと閾値ベクトルself.shiftの初期値が設定される。また、活性化関数self.activatorは\textbf{ソフトマックス関数}に初期化される。

%#########################################
\subsection{dataモジュール}
処理の対象となるデータを表現したクラスを集めたモジュール。

\subsubsection{MnistDataBoxクラス}
MNISTデータセットを表すクラス。外部仕様としては後に記すMnistDataクラスの集合であるが、内部仕様としては単一MNISTデータを取り出す時にMnistDataクラスを生成する。また、バッチの生成も行う。

主なメソッドは以下の通りである。

\begin{itemize}
  \item getSingleData(self, num)

  インデックスを引数で指定し、単一のMNISTデータを表すMnistDataオブジェクトを取得する。

  \item shuffle(self, seed = 1)

  内部的なMNISTデータの保持順、インデックスをシャッフルする。単一MNISTデータやバッチを取り出す際に、シャッフルの前後ではインデックスが同一であっても別のデータを取り出すことができる。

  \item getImageBatch(self, batchSize, shift = 0)

  画像データをバッチ形式で取り出す。引数にはバッチサイズとバッチを取り出す基準となるインデックスを指定する。

  \item getAnswerVecotrBatch(self, batchSize, shift = 0)

  ラベルデータをバッチ形式で取り出す。引数にはバッチサイズとバッチを取り出す基準となるインデックスを指定する。

  \item getSize(self)

  MNISTデータセットのサイズ（格納しているMNISTデータの数）を返す。
\end{itemize}

%####
\subsubsection{MnistDataクラス}
MNIST単一データを表すクラス。

主なメソッドは以下の通り。

\begin{itemize}
  \item getImage(self)

  画像データを行列形式で取り出す。

  \item getAnswer(self)

  ラベルデータを取り出す。

  \item getAnswerAsVector(self, size = None)

  ラベルデータを、そのラベルに対応するインデックスが１、それ以外のインデックスが０のベクトル形式で取り出す。

\end{itemize}

\section{実行結果}

「\textgreater\textgreater」以下が入力、「Result. TotalLoss :」以下が出力である。

実行例を以下に示す。

\begin{verbatim}
  ....neural_network> python task2.py
  ### task2 ###
  batch mode
  start loading (testing data)
  finish loading
  Input random seed number of batch.
  select number.
  >> 0
  0 is selected.
  start

  Result.
  totalLoss : 2.30539980393

  continue?
  select yes or no.
  >> yes
  Input random seed number of batch.
  select number.
  >> 4
  4 is selected.
  start

  Result.
  totalLoss : 2.30575362662

  continue?
  select yes or no.
  >> no
  Bye.
\end{verbatim}

\section{工夫}
\begin{itemize}
  \item 全てのプログラムはオブジェクト指向・ドメインモデル方式で設計されている。これによりモジュールの再利用・拡張が容易になり、今後様々なニューラルネットを組み上げることができる。
  \item 各レイヤー内で使用される関数を分離している。これにより、容易に使用される関数を変更することができ、幅広い種類のニューラルネットを簡単に構成できる。
  \item 入出力を任意の回数行えるようなインターフェースとした。
  \item バッチは元データの行列をスライスして作成している。バッチ作成にforループを使用しないことで高速化を図った。
\end{itemize}

\section{問題点}
一般化を心がける余り、コードがやや肥大化してしまっている。応用課題に取り組む時まで一般化の恩恵は受けられないので、効果が今のところ実感できない。

\end{document}
