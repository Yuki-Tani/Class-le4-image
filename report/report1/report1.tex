\documentclass{ujarticle} %upLaTeXで動いているのでunicode対応のこのクラス
\usepackage{xcolor}
\usepackage[dvipdfmx]{graphicx}
\usepackage{float}
\usepackage{ascmac}
% todo command
\newcommand\todo[1]{\textcolor{red}{[todo] #1}}
\newcommand\tips[1]{\textcolor{blue}{#1}}

\begin{document}
\title{計算機科学実験及演習4 \\ \bf 画像認識 第１回レポート}
% ↓ここに自分の氏名を記入
\author{谷　勇輝 \\ \\入学年 平成27年 \\ 学籍番号 1029-27-2870}
\西暦
\date{締切日: 2017年12月22日\\ 提出日: 2017年12月21日}
\maketitle
\newpage

% レポートには，課題内容，作成したプログラムの説明，実行結果，工夫点，問題点を記述して下さい．これとは別にソースファイルを添付して下さい．レポートは報告書の形式を要求します．くだけた文体，支離滅裂な文章については低評価となります

\begin{itembox}[l]{[課題1] 3層ニューラルネットワークの構築（順伝播）}
  MNIST の画像1枚を入力とし，3層ニューラルネットワークを用いて，0～9の値のうち1つを出力するプログラムを作成せよ．
  \begin{itemize}
    \item キーボードから 0～9999 の整数を入力 i として受け取り，0～9 の整数を標準出力に出力すること．
    \item MNIST のテストデータ 10000 枚の画像のうち i 番目の画像を入力画像として用いる．
    \item MNIST の画像サイズ（28 × 28），画像枚数（10000 枚），クラス数（C = 10）は既知とする．ただし，後々の改良のため変更可能な仕様にしておくことを薦める．
    \item  中間層のノード数 M は自由に決めて良い.
    \item 重み W(1), W(2), b(1), b(2) については乱数で決定すること．ここでは，手前の層のノード数を N として 1/N を分散とする平均 0 の正規分布で与えることとする．適切な重みを設定しないため，課題 1 の段階では入力に対してデタラメな認識結果を返す．ただし，実行する度に同じ結果を出力するよう乱数のシードを固定すること.
  \end{itemize}
\end{itembox}

\section{作成したプログラム}
\subsection{プログラム構成}
今回作成したプログラムは以下のモジュール、クラスからなる。

オブジェクト指向・ドメインモデルに基づいた適切なモジュール設計により、今後の開発がよりスムーズになるよう注意した。

\begin{verbatim}
neural_network
 ┣ ioex.py            … 入出力系モジュール
 ┃ ┣ InputManager       … 入力を担当するクラス
 ┃ ┗ OutputManager      … 出力を担当するクラス
 ┣ data.py            … プログラムで扱う基本データ系モジュール
 ┃ ┣ MnistDataBox       … MNISTデータ群を表すクラス
 ┃ ┗ MnistData          … 単一のMNISTデータを表すクラス
 ┣ layer.py           … ニューラルネットの層モジュール
 ┃ ┣ Layer              … 基本の層構造を表すベースクラス
 ┃ ┣ InputLayer         … 入力層を表すクラス
 ┃ ┣ ConversionLayer    … 変換を行う層を表すクラス
 ┃ ┣ HiddenLayer        … 中間層を表すクラス
 ┃ ┗ OutputLayer        … 出力層を表すクラス
 ┣ util.py            … 汎用関数や汎用クラスを集めたモジュール
 ┃ ┣ ComplexMaker       … numpyのrandomを扱うクラス
 ┃ ┗ 各種数学メソッド
 ┣ test.py            … 動作確認・テスト用モジュール
 ┗ task1.py           … 課題１プログラム
MNIST
\end{verbatim}


\subsection{メインプログラム}

課題１のメインプログラムtest1.pyは以下の通りである。
入力はMNISTデータの番号、出力は学習前のランダムな重みによる３層ニューラルネットワークの出力である。追加の機能として、入出力を任意回数行えるように変更した。

\begin{itembox}[l]{test1.py}
  \begin{verbatim}
 1    import ioex
 2    import layer
 3    import util
 4
 5    print("### task1 ###")
 6
 7    # 入出力準備
 8    inputM = ioex.InputManager()
 9    outputM = ioex.OutputManager()
10    testingData = inputM.getMnistsTestingData()
11
12    # ３層ニューラルネットワーク
13    inputLayer = layer.InputLayer(28*28)
14    hiddenLayer = layer.HiddenLayer(50,inputLayer,1,1)
15    outputLayer = layer.OutputLayer(10,hiddenLayer,1,1)
16
17    #入出力
18    loop = True
19    while(loop):
20        # 入力
21        targetNum = inputM.selectNumber()
22        sample = testingData.getSingleData(targetNum)
23        inputLayer.setInput(sample.getImage())
24
25        # 出力
26        result = outputLayer.calculate()
27        outputM.printMaxLikelihood(result)
28        print(result)
29
30        print("\ncontinue?")
31        loop = inputM.selectYesOrNo()
32
33    print("Bye.")
  \end{verbatim}
\end{itembox}

13行目から15行目では、３層ニューラルネットワークの作成を行っている。各Layerクラスにおいて、第１引数はその層における次元の数(ニューロンの数)、第２引数は前の層のインスタンス、第３引数はニューロンの入力にかかる重みの初期値を決定する乱数シード、第４引数はニューロンの入力に加算される閾値の初期値を決定する乱数シードである。各Layerインスタンスは生成された時にコンストラクタによって初期化され、その際に各重みの値は指定した乱数シードに基づきランダムに決定される。

入力後のプログラムの流れを示す。まず、23行目の InputLayer\#setInputメソッド によってニューラルネットに入力がセットされる。次に、26行目のOutputLayer\#calculateメソッド によってニューラルネットワーク内の値が順方向に更新される。最後に、27行目のOutputManager\#printMaxLikelihoodメソッドによって 出力層の結果の中で最大の値をもつニューロンを特定しその番号を出力している。

\subsection{layerモジュール}
ニューラルネットワークの部品となるlayerモジュール内のクラスはこのプログラム群において重要な役割を果たしている。その詳細を以下に示す。

\subsubsection{Layerクラス}
このモジュール内の全てのLayerクラスのベースクラスであり、他のLayerはこのクラスを必ず継承する。層についての基本の機能を備えている。

主なインスタンス変数は以下の通りである。
\begin{itemize}
  \item self.dimension

  層に含まれるニューロンの数、すなわち層の次元数を表す。コンストラクタの引数によって初期化される。

  \item self.output

  層の出力を表す。caluculateメソッドによって更新され、現在の最新の出力情報を保持する。初期値は全て０である。

\end{itemize}

また、主なメソッドは以下の通りである。

\begin{itemize}
  \item \_\_init\_\_(self, dimension)

  インスタンスを作成し値を初期化する。層に含まれるニューロンの数(次元数)はここで決定する。

  \item calculate(self)

  現在の最新の情報を使用して出力を更新する。更新後の値を返す。

  \item getOutput(self)

  現在の最新の出力情報を返す。このメソッドでは出力値の更新は行われない。

  \item confirmParameters(self)

  層についての情報を標準出力に表示する。

  \item getDimension(self)

  層が含むニューロンの数、すなわち次元数を返す。
\end{itemize}

\subsubsection{InputLayer}
入力層を表すクラス。Layerクラスを継承する。

重要なインスタンス変数は以下の通りである。
\begin{itemize}
  \item self.input

  入力を表す変数。初期値は全て０である。setInputメソッドによって設定する。
\end{itemize}

重要なメソッドは以下の通りである。

\begin{itemize}
  \item setInput(self, inputData)

  入力を設定する。引数に与えられた配列は１次元に圧縮される。

  \item calculate(self)

  Layerクラスのメソッドをオーバーライドしている。現在の入力の値をそのまま出力として設定する。
\end{itemize}

\subsubsection{ConversionLayer}
入力に対し重みをつけ、さらに何らかの活性関数を適応したものを出力とする層、すなわち何らかの変換を行う層を表す。具体的な実装としては後に示すHiddenLayerやOutputLayerがある。

重要なインスタンス変数は以下の通りである。
\begin{itemize}
  \item self.prevLayer

  直前の層のインスタンスを保持する。コンストラクタの引数によって初期化される。

  \item self.weightSize

  入力に掛けられる重み行列のサイズを表したタプル。前の層の次元情報とこの層の次元情報から自動的に初期化される。

  \item self.shiftSize

  入力に加算される閾値ベクトルのサイズを表したタプル。この層の次元情報から自動的に初期化される。

  \item self.weight

  入力に掛けられる重み行列を表す変数。このクラスでは単位行列に初期化される。

  \item self.shift

  入力に加算される閾値ベクトルを表す変数。このクラスではゼロベクトルに初期化される。

  \item self.activator

  重み付け後に適応される活性関数をあらわす変数。このクラスでは恒等変換に初期化される。
\end{itemize}

重要なメソッドは以下の通りである。

\begin{itemize}
  \item calculate(self)

  Layerクラスのメソッドをオーバーライドしている。まず前の層のcalculateを行い、その最新の出力を入力として受け取る。その入力にself.weightをかけ、self.shiftを加算することで重み付けを行う。最後に活性関数self.activatorを適応し得られた値を最新の出力として更新する。また、更新後の値を返す。この再帰的なcalculateの呼び出しにより、ニューラルネットワークの最後の層のcalculateを呼び出すことでネットワーク全ての値を順方向に更新できることになる。

  \item setWeight(self, weight)

  重み行列self.weightを設定する。この層に合わない形の行列が入力された場合には設定は行われず、警告文が標準出力に表示される。

  \item setShift(self, shift)

  閾値ベクトルself.shiftを設定する。この層に合わない形のベクトルが入力された場合には設定は行われず、警告文が標準出力に表示される。

  \item setActivator(self, function)

  活性化関数self.activatorを設定する。関数型以外の値が入力された場合には設定は行われず、警告文が標準出力に表示される。
\end{itemize}

\subsubsection{HiddenLayer}
中間層を表すクラス。変換を行う層であるので、ConversionLayerを継承する。

ConversionLayerとの違いは、その初期化の内容である。こちらのクラスでは、コンストラクタで２つの乱数シードweightSeedとshiftSeedを要求し（デフォルトの値は１）、その乱数シードを使って生成したランダム値に基づいて重み行列self.weightと閾値ベクトルself.shiftの初期値が設定される。また、活性化関数self.activatorは\textbf{シグモイド関数}に初期化される。

\subsubsection{OutputLayer}
出力層を表すクラス。変換を行う層であるので、ConversionLayerを継承する。

ConversionLayerとの違いは、その初期化の内容である。こちらのクラスでは、コンストラクタで２つの乱数シードweightSeedとshiftSeedを要求し（デフォルトの値は１）、その乱数シードを使って生成したランダム値に基づいて重み行列self.weightと閾値ベクトルself.shiftの初期値が設定される。また、活性化関数self.activatorは\textbf{ソフトマックス関数}に初期化される。


\section{実行結果}

「\textgreater\textgreater」以下が入力、「Recognition Result :」以下が出力である。なお、今後の開発のことを考慮し、その他のデータも一部表示する仕様にした。

実行例を以下に示す。

\begin{verbatim}
  ...neural_network> python task1.py
  ### task1 ###
  start loading (testing data)
  finish loading
  select number.
  >> 1
  1 is selected.

  Recognition Result : 6
  (likelihood : 0.113937661163)

  [[ 0.10316129]
   [ 0.10057583]
   [ 0.09511654]
   [ 0.10266565]
   [ 0.09758801]
   [ 0.08950398]
   [ 0.11393766]
   [ 0.09026591]
   [ 0.11292604]
   [ 0.09425909]]

  continue?
  select yes or no.
  >> yes
  select number.
  >> 200
  200 is selected.

  Recognition Result : 8
  (likelihood : 0.109248250636)

  [[ 0.1038663 ]
   [ 0.10188789]
   [ 0.0975031 ]
   [ 0.09667546]
   [ 0.10470781]
   [ 0.09053544]
   [ 0.10627111]
   [ 0.09216106]
   [ 0.10924825]
   [ 0.09714358]]

  continue?
  select yes or no.
  >> no
  Bye.
\end{verbatim}

\section{工夫}
\begin{itemize}
  \item 全てのプログラムはオブジェクト指向・ドメインモデル方式で設計されている。これによりモジュールの再利用・拡張が容易になり、今後様々なニューラルネットを組み上げることができる。
  \item 入出力を任意の回数行えるようなインターフェースとした。
  \item 乱数シード、層の次元数等の定数は容易に変更が可能となるように設計した。
\end{itemize}

\section{問題点}
他の人より進捗がやや遅い。綺麗な設計でも素早く作れるようになりたいものである。


\end{document}
